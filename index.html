<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Multi-Armed Bandit</title>
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="style.css">
  <script src="d3.min.js"></script>
</head>

<body>
  <h1>Multi-Armed Bandit</h1>
  <div id="container">
    <div>
      <h2>Arms and expected reward</h2>
      <svg id="paths"></svg>
      <br>
      <div id="buttons">
        <button onclick="reset()">Reset</button>
        <div>
          <label>Number of steps:</label>
          <input id="steps" type="number" max="3000" onKeyUp="if(this.value>3000){this.value='3000';}"></button>
          <button onclick="n_steps(steps.value);disable()">Run steps</button>
        </div>
        <button onclick="step();disable()">Next Step</button>
      </div>
    </div>
    <div>
      <h2>True mean reward</h2>
      <div id="means"></div>
    </div>
    <div>
      <h2>Expected reward over time</h2>
      <svg id="scatter"></svg>
    </div>
  </div>
    <script src="tree.js"></script>
    <script src="scatter.js"></script>
    <script src="bandit.js"></script>
    <script>
      function disable() {
        let b = document.getElementsByTagName('button');
        for (let i = 0; i < b.length; i++) {
          b[i].setAttribute('disabled', 'disabled');
          setTimeout(() => {
            b[i].removeAttribute('disabled');
          }, 1500);
        }
      }
      </script>
  <p>
    This visualisation represents an agent learning which is the best path to take to maximize his reward.<br>
    Each path gives a random reward following a normal distribution of a fixed mean given by the table.<br>
    To make sure the agent don't always try the same path it will try a sub optimal path 10% of the time in order to explore other paths.<br>
    You can also notice that each path starts with an expected reward of 5 to force the agent to try every path during the first 10 steps.<br>
  </p>
</body>

</html>